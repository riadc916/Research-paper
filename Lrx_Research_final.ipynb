{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riadc916/Research-paper/blob/main/Lrx_Research_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-DThjBT_JTh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers scikit-learn pandas\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "FILE_NAME = \"Lrx-data - loan_dataset_1000_consistent.csv\"\n",
        "\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bd45a8a"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgsNlPd1ARaQ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(FILE_NAME)\n",
        "\n",
        "df = df.dropna(subset=['loan_comment', 'classification_label'])\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "print(f\"Total number of samples: {len(df)}\")\n",
        "print(\"--- Counts of Original Classification Labels ---\")\n",
        "print(df['classification_label'].value_counts())\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['target_id'] = le.fit_transform(df['classification_label'])\n",
        "\n",
        "print(\"\\n--- Encoded Classification Labels (ID Map) ---\")\n",
        "target_map = dict(zip(df['target_id'], df['classification_label']))\n",
        "print(target_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8raSaqzFR6Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['loan_comment'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "dataset_to_tokenize = dataset.select_columns(['loan_comment', 'target_id'])\n",
        "\n",
        "tokenized_dataset = dataset_to_tokenize.map(tokenize_function, batched=True)\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"target_id\", \"labels\")\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns([\"loan_comment\"])\n",
        "\n",
        "\n",
        "train_val_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_val_dataset = train_val_test_split['train']\n",
        "test_dataset = train_val_test_split['test']\n",
        "\n",
        "train_val_split = train_val_dataset.train_test_split(test_size=0.125, seed=42)\n",
        "train_dataset = train_val_split['train']\n",
        "val_dataset = train_val_split['test']\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Validation set size: {len(val_dataset)}\")\n",
        "print(f\"Test set size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwGAQrpcF95Y"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate evaluate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmjxdnmfHvAH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "FILE_NAME = \"Lrx-data - loan_dataset_1000_consistent.csv\"\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(FILE_NAME)\n",
        "\n",
        "    df = df.dropna(subset=['loan_comment', 'classification_label'])\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    df['target_id'] = le.fit_transform(df['classification_label'])\n",
        "\n",
        "    NUM_LABELS = len(le.classes_)\n",
        "\n",
        "    print(f\"MODEL_NAME successfully defined: {MODEL_NAME}\")\n",
        "    print(f\"NUM_LABELS successfully defined: {NUM_LABELS}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during data loading/encoding: {e}. Please ensure the CSV file is uploaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Od13mU3IKvK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "FILE_NAME = \"Lrx-data - loan_dataset_1000_consistent.csv\"\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "\n",
        "df = pd.read_csv(FILE_NAME)\n",
        "df = df.dropna(subset=['loan_comment', 'classification_label'])\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['target_id'] = le.fit_transform(df['classification_label'])\n",
        "NUM_LABELS = len(le.classes_)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['loan_comment'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset_to_tokenize = dataset.select_columns(['loan_comment', 'target_id'])\n",
        "\n",
        "tokenized_dataset = dataset_to_tokenize.map(tokenize_function, batched=True)\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"target_id\", \"labels\")\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.remove_columns([\"loan_comment\"])\n",
        "\n",
        "train_val_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_val_dataset = train_val_test_split['train']\n",
        "test_dataset = train_val_test_split['test']\n",
        "\n",
        "train_val_split = train_val_dataset.train_test_split(test_size=0.125, seed=42)\n",
        "train_dataset = train_val_split['train']\n",
        "val_dataset = train_val_split['test']\n",
        "\n",
        "print(f\"Data loading and splitting complete. Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, NUM_LABELS: {NUM_LABELS}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate -U\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "FILE_NAME = \"Lrx-data - loan_dataset_1000_consistent.csv\"\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(FILE_NAME)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find the file {FILE_NAME}. Please ensure it is in the current directory.\")\n",
        "    raise\n",
        "\n",
        "df = df.dropna(subset=['loan_comment', 'classification_label'])\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['target_id'] = le.fit_transform(df['classification_label'])\n",
        "NUM_LABELS = len(le.classes_)\n",
        "\n",
        "\n",
        "causal_le = LabelEncoder()\n",
        "df['causal_id'] = causal_le.fit_transform(df['causal_ratio'])\n",
        "NUM_CAUSAL_LABELS = len(causal_le.classes_)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['loan_comment'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "dataset_mtl = Dataset.from_pandas(df)\n",
        "dataset_to_tokenize_mtl = dataset_mtl.select_columns(['loan_comment', 'target_id', 'causal_id'])\n",
        "\n",
        "dataset_to_tokenize_mtl = dataset_to_tokenize_mtl.rename_column(\"target_id\", \"labels\")\n",
        "\n",
        "tokenized_dataset_mtl = dataset_to_tokenize_mtl.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "cols_to_remove = [\"loan_comment\"]\n",
        "if \"__index_level_0__\" in tokenized_dataset_mtl.column_names:\n",
        "    cols_to_remove.append(\"__index_level_0__\")\n",
        "\n",
        "tokenized_dataset_mtl = tokenized_dataset_mtl.remove_columns(cols_to_remove)\n",
        "\n",
        "mtl_train_val_test_split = tokenized_dataset_mtl.train_test_split(test_size=0.2, seed=42)\n",
        "mtl_train_val_dataset = mtl_train_val_test_split['train']\n",
        "mtl_test_dataset = mtl_train_val_test_split['test']\n",
        "\n",
        "mtl_train_split = mtl_train_val_dataset.train_test_split(test_size=0.125, seed=42)\n",
        "mtl_train_dataset = mtl_train_split['train']\n",
        "mtl_val_dataset = mtl_train_split['test']\n",
        "\n",
        "print(\"\\n--- All Prerequisites and Multi-Task Datasets are Ready! ---\")\n",
        "print(f\"MTL Training set size: {len(mtl_train_dataset)}, Classification Labels: {NUM_LABELS}, Causal Labels: {NUM_CAUSAL_LABELS}\")"
      ],
      "metadata": {
        "id": "qs_-y3eF4qhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TIw5NpuG2Ft"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install evaluate -U\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "FILE_NAME = \"Lrx-data - loan_dataset_1000_consistent.csv\"\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(FILE_NAME)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find the file {FILE_NAME}. Please ensure it is in the current directory.\")\n",
        "    raise\n",
        "\n",
        "df = df.dropna(subset=['loan_comment', 'classification_label'])\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['target_id'] = le.fit_transform(df['classification_label'])\n",
        "NUM_LABELS = len(le.classes_)\n",
        "\n",
        "causal_le = LabelEncoder()\n",
        "df['causal_id'] = causal_le.fit_transform(df['causal_ratio'])\n",
        "NUM_CAUSAL_LABELS = len(causal_le.classes_)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['loan_comment'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "dataset_mtl = Dataset.from_pandas(df)\n",
        "dataset_to_tokenize_mtl = dataset_mtl.select_columns(['loan_comment', 'target_id', 'causal_id'])\n",
        "\n",
        "dataset_to_tokenize_mtl = dataset_to_tokenize_mtl.rename_column(\"target_id\", \"labels\")\n",
        "\n",
        "tokenized_dataset_mtl = dataset_to_tokenize_mtl.map(tokenize_function, batched=True)\n",
        "\n",
        "cols_to_remove = [\"loan_comment\"]\n",
        "if \"__index_level_0__\" in tokenized_dataset_mtl.column_names:\n",
        "    cols_to_remove.append(\"__index_level_0__\")\n",
        "\n",
        "tokenized_dataset_mtl = tokenized_dataset_mtl.remove_columns(cols_to_remove)\n",
        "\n",
        "mtl_train_val_test_split = tokenized_dataset_mtl.train_test_split(test_size=0.2, seed=42)\n",
        "mtl_train_val_dataset = mtl_train_val_test_split['train']\n",
        "mtl_test_dataset = mtl_train_val_test_split['test']\n",
        "\n",
        "mtl_train_split = mtl_train_val_dataset.train_test_split(test_size=0.125, seed=42)\n",
        "mtl_train_dataset = mtl_train_split['train']\n",
        "mtl_val_dataset = mtl_train_split['test']\n",
        "\n",
        "print(\"\\n--- All Prerequisites and Multi-Task Datasets are Ready! ---\")\n",
        "print(f\"MTL Training set size: {len(mtl_train_dataset)}, Classification Labels: {NUM_LABELS}, Causal Labels: {NUM_CAUSAL_LABELS}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from torch import nn\n",
        "from transformers import BertPreTrainedModel, BertModel, TrainingArguments, Trainer, AutoConfig\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "class BertMultiTaskClassifier(BertPreTrainedModel):\n",
        "    \"\"\"\n",
        "    A BERT model with two classification heads:\n",
        "    Task 1: Classification Label, Task 2: Causal Ratio ID.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        super().__init__(config, *args, **kwargs)\n",
        "\n",
        "        self.num_labels = config.num_labels\n",
        "        self.num_causal_labels = config.num_causal_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
        "        self.causal_classifier = nn.Linear(config.hidden_size, self.num_causal_labels)\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        labels=None,\n",
        "        causal_id=None,\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs.pooler_output\n",
        "        classification_logits = self.classifier(pooled_output)\n",
        "        causal_logits = self.causal_classifier(pooled_output)\n",
        "\n",
        "        total_loss = None\n",
        "        if labels is not None and causal_id is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss_classification = loss_fct(classification_logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            loss_causal = loss_fct(causal_logits.view(-1, self.num_causal_labels), causal_id.view(-1))\n",
        "\n",
        "            total_loss = loss_classification + loss_causal\n",
        "\n",
        "        return (total_loss, classification_logits, causal_logits)\n",
        "\n",
        "\n",
        "\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "config.num_labels = NUM_LABELS\n",
        "config.num_causal_labels = NUM_CAUSAL_LABELS\n",
        "\n",
        "\n",
        "print(\"Loading Multi-Task BERT Model...\")\n",
        "mtl_model = BertMultiTaskClassifier.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "\n",
        "mtl_training_args = TrainingArguments(\n",
        "    output_dir=\"./results_mtl\",\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_mtl',\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "def compute_mtl_metrics(eval_pred):\n",
        "    classification_logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, tuple) else eval_pred.predictions\n",
        "    labels = eval_pred.label_ids[0] if isinstance(eval_pred.label_ids, tuple) else eval_pred.label_ids\n",
        "\n",
        "    accuracy = evaluate.load(\"accuracy\").compute(predictions=np.argmax(classification_logits, axis=-1), references=labels)\n",
        "    f1 = evaluate.load(\"f1\").compute(predictions=np.argmax(classification_logits, axis=-1), references=labels, average='weighted')\n",
        "\n",
        "    return {\"accuracy\": accuracy[\"accuracy\"], \"f1_weighted\": f1[\"f1\"]}\n",
        "\n",
        "\n",
        "mtl_trainer = Trainer(\n",
        "    model=mtl_model,\n",
        "    args=mtl_training_args,\n",
        "    train_dataset=mtl_train_dataset,\n",
        "    eval_dataset=mtl_val_dataset,\n",
        "    compute_metrics=compute_mtl_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Starting the Multi-Task Fine-tuning process...\")\n",
        "mtl_trainer.train()\n",
        "print(\"MTL Training complete!\")"
      ],
      "metadata": {
        "id": "hL4v8iw_CGv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install evaluate -U\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from datasets import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, BertPreTrainedModel, BertModel, TrainingArguments, Trainer, AutoConfig\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "FILE_NAME = \"Lrx-data - loan_dataset_1000_consistent.csv\"\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(FILE_NAME)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find the file {FILE_NAME}. Please ensure it is in the current directory.\")\n",
        "    raise\n",
        "\n",
        "df = df.dropna(subset=['loan_comment', 'classification_label'])\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['target_id'] = le.fit_transform(df['classification_label'])\n",
        "NUM_LABELS = len(le.classes_)\n",
        "\n",
        "causal_le = LabelEncoder()\n",
        "df['causal_id'] = causal_le.fit_transform(df['causal_ratio'])\n",
        "NUM_CAUSAL_LABELS = len(causal_le.classes_)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['loan_comment'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "dataset_mtl = Dataset.from_pandas(df)\n",
        "dataset_to_tokenize_mtl = dataset_mtl.select_columns(['loan_comment', 'target_id', 'causal_id'])\n",
        "dataset_to_tokenize_mtl = dataset_to_tokenize_mtl.rename_column(\"target_id\", \"labels\")\n",
        "tokenized_dataset_mtl = dataset_to_tokenize_mtl.map(tokenize_function, batched=True)\n",
        "\n",
        "cols_to_remove = [\"loan_comment\"]\n",
        "if \"__index_level_0__\" in tokenized_dataset_mtl.column_names:\n",
        "    cols_to_remove.append(\"__index_level_0__\")\n",
        "tokenized_dataset_mtl = tokenized_dataset_mtl.remove_columns(cols_to_remove)\n",
        "\n",
        "mtl_train_val_test_split = tokenized_dataset_mtl.train_test_split(test_size=0.2, seed=42)\n",
        "mtl_train_val_dataset = mtl_train_val_test_split['train']\n",
        "mtl_test_dataset = mtl_train_val_test_split['test']\n",
        "\n",
        "mtl_train_split = mtl_train_val_dataset.train_test_split(test_size=0.125, seed=42)\n",
        "mtl_train_dataset = mtl_train_split['train']\n",
        "mtl_val_dataset = mtl_train_split['test']\n",
        "\n",
        "print(\"--- Data Preparation Complete ---\")\n",
        "print(f\"MTL Training set size: {len(mtl_train_dataset)}, Causal Labels: {NUM_CAUSAL_LABELS}\")\n",
        "\n",
        "\n",
        "class BertMultiTaskClassifier(BertPreTrainedModel):\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        super().__init__(config, *args, **kwargs)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.num_causal_labels = config.num_causal_labels\n",
        "        self.bert = BertModel(config)\n",
        "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
        "        self.causal_classifier = nn.Linear(config.hidden_size, self.num_causal_labels)\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, causal_id=None):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        classification_logits = self.classifier(pooled_output)\n",
        "        causal_logits = self.causal_classifier(pooled_output)\n",
        "        total_loss = None\n",
        "        if labels is not None and causal_id is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss_classification = loss_fct(classification_logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            loss_causal = loss_fct(causal_logits.view(-1, self.num_causal_labels), causal_id.view(-1))\n",
        "            total_loss = loss_classification + loss_causal\n",
        "        return (total_loss, classification_logits, causal_logits)\n",
        "\n",
        "\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "config.num_labels = NUM_LABELS\n",
        "config.num_causal_labels = NUM_CAUSAL_LABELS\n",
        "\n",
        "print(\"Loading Multi-Task BERT Model...\")\n",
        "mtl_model = BertMultiTaskClassifier.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "mtl_training_args = TrainingArguments(\n",
        "    output_dir=\"./results_mtl\", num_train_epochs=5, learning_rate=3e-5, per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8, warmup_steps=500, weight_decay=0.01, logging_dir='./logs_mtl',\n",
        "    logging_steps=100, report_to=\"none\"\n",
        ")\n",
        "\n",
        "def compute_mtl_metrics(eval_pred):\n",
        "    classification_logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, tuple) else eval_pred.predictions\n",
        "    labels = eval_pred.label_ids[0] if isinstance(eval_pred.label_ids, tuple) else eval_pred.label_ids\n",
        "\n",
        "    accuracy = evaluate.load(\"accuracy\").compute(predictions=np.argmax(classification_logits, axis=-1), references=labels)\n",
        "    f1 = evaluate.load(\"f1\").compute(predictions=np.argmax(classification_logits, axis=-1), references=labels, average='weighted')\n",
        "\n",
        "    return {\"accuracy\": accuracy[\"accuracy\"], \"f1_weighted\": f1[\"f1\"]}\n",
        "\n",
        "\n",
        "mtl_trainer = Trainer(\n",
        "    model=mtl_model, args=mtl_training_args, train_dataset=mtl_train_dataset, eval_dataset=mtl_val_dataset,\n",
        "    compute_metrics=compute_mtl_metrics,\n",
        ")\n",
        "\n",
        "print(\"Starting the Multi-Task Fine-tuning process...\")\n",
        "mtl_trainer.train()\n",
        "print(\"MTL Training complete!\")"
      ],
      "metadata": {
        "id": "CiYrysIJ3L3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Final MTL Evaluation on Test Set ---\")\n",
        "\n",
        "mtl_results = mtl_trainer.evaluate(mtl_test_dataset)\n",
        "print(mtl_results)"
      ],
      "metadata": {
        "id": "knRpg4y73QcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "FILE_NAME = \"Lrx-data - loan_dataset_1000_consistent.csv\"\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "\n",
        "df = pd.read_csv(FILE_NAME)\n",
        "df = df.dropna(subset=['loan_comment', 'classification_label', 'causal_ratio'])\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['target_id'] = le.fit_transform(df['classification_label'])\n",
        "NUM_LABELS = len(le.classes_)\n",
        "\n",
        "causal_le = LabelEncoder()\n",
        "df['causal_id'] = causal_le.fit_transform(df['causal_ratio'])\n",
        "NUM_CAUSAL_LABELS = len(causal_le.classes_)\n",
        "\n",
        "SUBSTITUTIONS = {\n",
        "    \"Approved\": [\"Sanctioned\", \"Accepted\", \"Cleared\", \"Verified\"],\n",
        "    \"Rejected\": [\"Declined\", \"Denied\", \"Turned down\", \"Refused\"],\n",
        "    \"High\": [\"Excellent\", \"Strong\", \"Good\", \"Great\"],\n",
        "    \"Low\": [\"Poor\", \"Weak\", \"Insufficient\", \"Bad\"],\n",
        "    \"Score\": [\"Rating\", \"Standing\", \"Grade\"],\n",
        "    \"Risk\": [\"Danger\", \"Exposure\", \"Threat\"],\n",
        "    \"CIBIL\": [\"Credit Score\", \"Financial Rating\"],\n",
        "    \"Missing\": [\"Absent\", \"Unavailable\", \"Lacking\"],\n",
        "    \"Issue\": [\"Problem\", \"Concern\", \"Matter\"],\n",
        "    \"Proof of ownership\": [\"Title documents\", \"Property deeds\", \"Legal papers\"],\n",
        "}\n",
        "\n",
        "def augment_comment(comment):\n",
        "    new_comment = comment\n",
        "    comment_words = set(re.findall(r'\\b\\w+\\b', comment))\n",
        "    substitutable_words = [word for word in comment_words if word in SUBSTITUTIONS]\n",
        "\n",
        "    num_substitutions = random.choice([1, 2])\n",
        "    words_to_substitute = random.sample(substitutable_words, min(num_substitutions, len(substitutable_words)))\n",
        "\n",
        "    for word in words_to_substitute:\n",
        "        replacement = random.choice(SUBSTITUTIONS[word])\n",
        "\n",
        "        new_comment = re.sub(r'\\b' + re.escape(word) + r'\\b', replacement, new_comment, 1)\n",
        "\n",
        "    return new_comment\n",
        "\n",
        "new_rows = []\n",
        "AUGMENTATION_FACTOR = 4\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    original_comment = row['loan_comment']\n",
        "\n",
        "    new_rows.append(row.to_dict())\n",
        "\n",
        "    for i in range(AUGMENTATION_FACTOR):\n",
        "        augmented_comment = augment_comment(original_comment)\n",
        "\n",
        "        new_row_dict = {\n",
        "            'loan_comment': augmented_comment,\n",
        "\n",
        "            'loan_status': row['loan_status'],\n",
        "            'classification_label': row['classification_label'],\n",
        "            'causal_ratio': row['causal_ratio'],\n",
        "            'target_id': row['target_id'],\n",
        "            'causal_id': row['causal_id'],\n",
        "            'cibil_score': row['cibil_score'],\n",
        "            'language': row['language']\n",
        "        }\n",
        "        new_rows.append(new_row_dict)\n",
        "\n",
        "df_augmented = pd.DataFrame(new_rows)\n",
        "df_augmented = df_augmented.reset_index(drop=True)\n",
        "\n",
        "print(f\"Original Data Size: {len(df)}\")\n",
        "print(f\"Augmented Data Size: {len(df_augmented)}\")\n",
        "print(f\"First 5 Augmented Samples:\")\n",
        "print(df_augmented[['loan_comment', 'classification_label', 'causal_ratio']].head())\n",
        "df = df_augmented.copy()"
      ],
      "metadata": {
        "id": "0eH7qHe1L8d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['loan_comment'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "dataset_mtl = Dataset.from_pandas(df)\n",
        "dataset_to_tokenize_mtl = dataset_mtl.select_columns(['loan_comment', 'target_id', 'causal_id'])\n",
        "\n",
        "dataset_to_tokenize_mtl = dataset_to_tokenize_mtl.rename_column(\"target_id\", \"labels\")\n",
        "\n",
        "print(\"Tokenizing 5000 augmented samples...\")\n",
        "tokenized_dataset_mtl = dataset_to_tokenize_mtl.map(tokenize_function, batched=True)\n",
        "\n",
        "cols_to_remove = [\"loan_comment\"]\n",
        "if \"__index_level_0__\" in tokenized_dataset_mtl.column_names:\n",
        "    cols_to_remove.append(\"__index_level_0__\")\n",
        "\n",
        "tokenized_dataset_mtl = tokenized_dataset_mtl.remove_columns(cols_to_remove)\n",
        "\n",
        "\n",
        "mtl_train_val_test_split = tokenized_dataset_mtl.train_test_split(test_size=0.1, seed=42)\n",
        "mtl_train_val_dataset = mtl_train_val_test_split['train']\n",
        "mtl_test_dataset = mtl_train_val_test_split['test']\n",
        "\n",
        "\n",
        "mtl_train_split = mtl_train_val_dataset.train_test_split(test_size=(1/9), seed=42)\n",
        "mtl_train_dataset = mtl_train_split['train']\n",
        "mtl_val_dataset = mtl_train_split['test']\n",
        "\n",
        "print(\"\\n--- Data Re-Preparation Complete (5000 Samples) ---\")\n",
        "print(f\"New MTL Training set size: {len(mtl_train_dataset)} (80%)\")\n",
        "print(f\"New Validation set size: {len(mtl_val_dataset)} (10%)\")\n",
        "print(f\"New Test set size: {len(mtl_test_dataset)} (10%)\")"
      ],
      "metadata": {
        "id": "FSv7fwqAMbx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from torch import nn\n",
        "from transformers import BertPreTrainedModel, BertModel, TrainingArguments, Trainer, AutoConfig\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "class BertMultiTaskClassifier(BertPreTrainedModel):\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        super().__init__(config, *args, **kwargs)\n",
        "\n",
        "        self.num_labels = config.num_labels\n",
        "        self.num_causal_labels = config.num_causal_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
        "        self.causal_classifier = nn.Linear(config.hidden_size, self.num_causal_labels)\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        labels=None,\n",
        "        causal_id=None,\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs.pooler_output\n",
        "        classification_logits = self.classifier(pooled_output)\n",
        "        causal_logits = self.causal_classifier(pooled_output)\n",
        "\n",
        "        total_loss = None\n",
        "        if labels is not None and causal_id is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss_classification = loss_fct(classification_logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            loss_causal = loss_fct(causal_logits.view(-1, self.num_causal_labels), causal_id.view(-1))\n",
        "\n",
        "            total_loss = loss_classification + (0.5 * loss_causal)\n",
        "\n",
        "        return (total_loss, classification_logits, causal_logits)\n",
        "\n",
        "\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "config.num_labels = NUM_LABELS\n",
        "config.num_causal_labels = NUM_CAUSAL_LABELS\n",
        "\n",
        "print(\"Loading Multi-Task BERT Model for 5000 samples...\")\n",
        "mtl_model = BertMultiTaskClassifier.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "\n",
        "mtl_training_args = TrainingArguments(\n",
        "    output_dir=\"./results_mtl_v3\",\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_mtl_v3',\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "def compute_mtl_metrics(eval_pred):\n",
        "    classification_logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, tuple) else eval_pred.predictions\n",
        "    labels = eval_pred.label_ids[0] if isinstance(eval_pred.label_ids, tuple) else eval_pred.label_ids\n",
        "\n",
        "    accuracy = evaluate.load(\"accuracy\").compute(predictions=np.argmax(classification_logits, axis=-1), references=labels)\n",
        "    f1 = evaluate.load(\"f1\").compute(predictions=np.argmax(classification_logits, axis=-1), references=labels, average='weighted')\n",
        "\n",
        "    return {\"accuracy\": accuracy[\"accuracy\"], \"f1_weighted\": f1[\"f1\"]}\n",
        "\n",
        "\n",
        "mtl_trainer = Trainer(\n",
        "    model=mtl_model, args=mtl_training_args, train_dataset=mtl_train_dataset, eval_dataset=mtl_val_dataset,\n",
        "    compute_metrics=compute_mtl_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Starting the Multi-Task Fine-tuning process with 5000 samples (Expected Performance Boost)...\")\n",
        "mtl_trainer.train()\n",
        "print(\"MTL Training complete!\")"
      ],
      "metadata": {
        "id": "zFRkfbQdMp8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Final MTL Evaluation on Test Set (V3: 5000 Samples) ---\")\n",
        "\n",
        "mtl_results_v3 = mtl_trainer.evaluate(mtl_test_dataset)\n",
        "print(mtl_results_v3)"
      ],
      "metadata": {
        "id": "lQgTf6JGXEPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mtl_trainer.save_model(\"./final_mtl_model_v3\")\n",
        "print(\"Final model saved successfully to ./final_mtl_model_v3\")"
      ],
      "metadata": {
        "id": "M8w4ehmkZGIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mtl_trainer.save_model(\"./final_mtl_model_v3_f1_450\")\n",
        "print(\"Final model saved successfully to ./final_mtl_model_v3_f1_450\")"
      ],
      "metadata": {
        "id": "MSsOGbqbaNoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_history = mtl_trainer.state.log_history\n",
        "print(f\"Total log entries: {len(training_history)}\")"
      ],
      "metadata": {
        "id": "ZfVp7BZnaVT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import evaluate\n",
        "\n",
        "\n",
        "\n",
        "log_df = pd.DataFrame(training_history)\n",
        "\n",
        "eval_df = log_df.dropna(subset=['eval_loss'])\n",
        "\n",
        "train_df = log_df.dropna(subset=['loss'])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_df['step'], train_df['loss'], label='Training Loss', color='darkblue')\n",
        "\n",
        "plt.plot(eval_df['step'], eval_df['eval_loss'], label='Validation Loss', color='red', marker='o')\n",
        "\n",
        "plt.title('Training and Validation Loss Curve (MTL V3: 5000 Samples)')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(eval_df['step'], eval_df['eval_f1_weighted'], label='Validation F1-Weighted', color='green', marker='x')\n",
        "\n",
        "plt.title('Validation F1-Weighted Score (MTL V3)')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('F1-Weighted Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nGenerating Predictions for Confusion Matrix...\")\n",
        "predictions = mtl_trainer.predict(mtl_test_dataset)\n",
        "classification_logits = predictions.predictions[0]\n",
        "predicted_labels = np.argmax(classification_logits, axis=-1)\n",
        "true_labels = mtl_test_dataset[\"labels\"]\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "class_names = [f'Class {i}' for i in range(NUM_LABELS)]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix (MTL V3)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nVisualization generation complete. Please use the generated charts in your paper.\")"
      ],
      "metadata": {
        "id": "gdpX6V4waiMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import evaluate\n",
        "\n",
        "\n",
        "log_df = pd.DataFrame(training_history)\n",
        "eval_df = log_df.dropna(subset=['eval_loss'])\n",
        "train_df = log_df.dropna(subset=['loss'])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_df['step'], train_df['loss'], label='Training Loss', color='darkblue')\n",
        "plt.plot(eval_df['step'], eval_df['eval_loss'], label='Validation Loss', color='red', marker='o')\n",
        "plt.title('Training and Validation Loss Curve (MTL V3: 5000 Samples)')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"loss_curve.pdf\", bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"1. 'loss_curve.pdf' successfully saved.\")\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(eval_df['step'], eval_df['eval_f1_weighted'], label='Validation F1-Weighted', color='green', marker='x')\n",
        "plt.title('Validation F1-Weighted Score (MTL V3)')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('F1-Weighted Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"f1_curve.pdf\", bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"2. 'f1_curve.pdf' successfully saved.\")\n",
        "\n",
        "\n",
        "predictions = mtl_trainer.predict(mtl_test_dataset)\n",
        "classification_logits = predictions.predictions[0]\n",
        "predicted_labels = np.argmax(classification_logits, axis=-1)\n",
        "true_labels = mtl_test_dataset[\"labels\"]\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "class_names = [f'Class {i}' for i in range(NUM_LABELS)]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix (MTL V3)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.savefig(\"confusion_matrix.pdf\", bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"3. 'confusion_matrix.pdf' successfully saved.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_qHwX4W_eNRK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNn5adyHbskaI4tJi5YUXom",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
